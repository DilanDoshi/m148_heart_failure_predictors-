{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e4ab7a",
   "metadata": {},
   "source": [
    "# Data Cleaning & Preprocessing Notebook 02\n",
    "This is where we will do any cleaning and standardizations to prepare for using ML models to predict DEATH_EVENT.  \n",
    "\n",
    "*Note:* Some of these steps are redundant/unnecessary but are show for 'full lifecycle coverage'. Sometimes these two note books (ie. 01,02) are combined into 1 for simplicity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ad8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set the style of the plots\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21f1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/heart_failure_clinical_records_dataset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d31f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
    "categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406c3385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "age                         0\n",
      "anaemia                     0\n",
      "creatinine_phosphokinase    0\n",
      "diabetes                    0\n",
      "ejection_fraction           0\n",
      "high_blood_pressure         0\n",
      "platelets                   0\n",
      "serum_creatinine            0\n",
      "serum_sodium                0\n",
      "sex                         0\n",
      "smoking                     0\n",
      "time                        0\n",
      "DEATH_EVENT                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Detect and handle missing values (already checked)\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Handle missing values - None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1454f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age                         0\n",
       "creatinine_phosphokinase    0\n",
       "ejection_fraction           0\n",
       "platelets                   0\n",
       "serum_creatinine            3\n",
       "serum_sodium                0\n",
       "time                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Outliers (already checked)\n",
    "quantile_1 = df[continuous_features].quantile(0.25)\n",
    "quantile_3 = df[continuous_features].quantile(0.75)\n",
    "iqr = quantile_3 - quantile_1\n",
    "lower_bound = quantile_1 - 1.5 * iqr\n",
    "upper_bound = quantile_3 + 1.5 * iqr\n",
    "\n",
    "outliers = (df[continuous_features] < lower_bound) | (df[continuous_features] > upper_bound)\n",
    "\n",
    "print(\"Outliers: \")\n",
    "outliers.sum()\n",
    "# Handle Outliers \n",
    "# Will skip this step as we did it in 01 EDA notebook\n",
    "# (ignoring the 3 extra outliers since they are being calculate on the cleaned dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea8508",
   "metadata": {},
   "source": [
    "### Standardization and Train Split \n",
    "Some of our ML models that we will be using require standardization of features. So we will first split our data, then standardize it.  \n",
    "\n",
    "Process:  \n",
    "- We will standardize only the continuous numerical features (age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time) using StandardScaler.  \n",
    "\n",
    "- Binary 0/1 variables were left unchanged. \n",
    "\n",
    "- Standardization was applied in a scikit-learn Pipeline, fit only on the training data, and then applied to the test set.   \n",
    "\n",
    "- This was necessary for scale-sensitive models such as logistic regression, SVM, k-NN, and neural networks. \n",
    "\n",
    "- Tree-based models do not require scaling, but using a consistent preprocessing pipeline prevents data leakage and maintains comparability across models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba55b07",
   "metadata": {},
   "source": [
    "### Models That Require Feature Scaling\n",
    "\n",
    "| **Model** | **Why It Needs Scaling** |\n",
    "|----------|---------------------------|\n",
    "| **Logistic Regression** | L2 regularization assumes all features are on the same scale; otherwise coefficients are penalized inconsistently. |\n",
    "| **SVM (Linear, RBF kernels)** | Uses distance and dot products; features with larger magnitude dominate the decision boundary. |\n",
    "| **k-NN** | Distance-based algorithmâ€”unscaled features distort nearest-neighbor calculations. |\n",
    "| **Neural Networks (PyTorch)** | Gradient-based optimization converges faster when inputs have similar scale; prevents exploding/vanishing gradients. |\n",
    "| **PCA** | Computes variance along components; features with larger variance dominate unless scaled. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bff77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validation, Test Split \n",
    "X = df.drop('DEATH_EVENT', axis=1)\n",
    "y = df['DEATH_EVENT']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4f4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer + Pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_features),\n",
    "        ('cat', 'passthrough', categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example pipeline with logistic regression, will redo this part in the ML notebook\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(penalty='l2', C=0.1, solver='liblinear',class_weight='balanced'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e054cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " [ 2.77865874e-16  6.45045780e-17  2.36930277e-16 -2.20183896e-16\n",
      "  4.69829017e-16 -3.91989358e-16  7.93902498e-17  4.91620112e-01\n",
      "  4.18994413e-01  3.68715084e-01  6.48044693e-01  3.35195531e-01]\n",
      "Std: \n",
      " [1.         1.         1.         1.         1.         1.\n",
      " 1.         0.49992977 0.49339446 0.4824565  0.47758012 0.47205877]\n"
     ]
    }
   ],
   "source": [
    "# Verify Feature Scaling\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "print(\"Mean: \\n\", X_train_scaled.mean(axis=0))\n",
    "print(\"Std: \\n\", X_train_scaled.std(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361c159",
   "metadata": {},
   "source": [
    "Since for continous features the mean ~ 0 and std ~ 1, scaling is working as intended!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f7708",
   "metadata": {},
   "source": [
    "### Note:\n",
    "The imbalance in the dataset in DEATH_EVENT is about 2.67:1 to died:survived. The imbalance is mild and using a process such as SMOTE may add unneccessary noise and can hurt generalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671c1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data to CSVs for access in next notebook (commented out to avoid overwriting)\n",
    "# X_train.to_csv(\"../data/X_train.csv\", index=False)\n",
    "# X_val.to_csv(\"../data/X_val.csv\", index=False)\n",
    "# y_train.to_csv(\"../data/y_train.csv\", index=False)\n",
    "# y_val.to_csv(\"../data/y_val.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
